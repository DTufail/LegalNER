{"cells":[{"cell_type":"markdown","metadata":{"id":"yjtlDsaCCnvJ"},"source":["## **Chapter 1: Introduction to BERT**\n","\n","## **Overview**\n","\n","BERT, or Bidirectional Encoder Representations from Transformers, represents a revolutionary approach in NLP. Developed by Google, BERT's key innovation is its deep bidirectionality, allowing the model to understand the context of a word based on all of its surroundings (left and right of the word).\n","\n","* Transformers: The backbone of BERT, transformers use an architecture that weights the influence of different words on each other's context. Unlike directional models, which read the text input sequentially (left-to-right or right-to-left), transformers read the entire sequence of words at once. This allows for more contextually informed representations of each word.\n","\n","* Pre-training and Fine-tuning: BERT is pre-trained on a large corpus of text and then fine-tuned for specific tasks. Pre-training involves learning general language representations from a large text dataset (like Wikipedia). Fine-tuning adapts these representations to specific NLP tasks using smaller task-specific datasets.\n","\n","* Bidirectionality: Traditional language models were either trained to understand language from left to right or vice versa. BERT, however, is trained to understand context in both directions simultaneously. This is achieved through a mechanism called Masked Language Model (MLM), where some percentage of the input tokens are masked at random, and the model learns to predict them based on the unmasked tokens."]},{"cell_type":"markdown","metadata":{"id":"YhZeoi0gajw6"},"source":["# **Chapter 2: Environment Setup**\n","## **Overview**\n","\n","Setting up the environment involves installing the necessary Python libraries and ensuring that your system or development environment is ready to handle the tasks of loading data, training models, and evaluating their performance.\n","\n","Installation of Libraries\n","\n","To work with the BERT model, specific libraries need to be installed that facilitate model loading, data manipulation, and computation. The primary library used is transformers, which provides access to BERT and other pre-trained models. Additionally, libraries like datasets help in loading and handling popular NLP datasets."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Fmb_hNi46SZP"},"outputs":[],"source":["!pip install transformers datasets tokenizers seqeval -q"]},{"cell_type":"markdown","metadata":{"id":"fcT8TbaZawFs"},"source":["This command installs the transformers library for accessing pre-trained models, datasets for dataset management, tokenizers for efficient text tokenization, and seqeval for evaluation metrics specific to sequence labeling tasks."]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"C0lPBXIT6VgY"},"outputs":[],"source":["import datasets\n","import numpy as np\n","from transformers import BertTokenizerFast\n","from transformers import DataCollatorForTokenClassification\n","from transformers import AutoModelForTokenClassification"]},{"cell_type":"markdown","metadata":{"id":"5dKBv7cia7yp"},"source":["* datasets: Handles loading and preparing datasets.\n","* numpy: Used for numerical operations.\n","* BertTokenizerFast: Provides a faster tokenization method.\n","* DataCollatorForTokenClassification: Prepares batches of data.\n","* AutoModelForTokenClassification: Loads a model pre-trained on token\n","* classification tasks."]},{"cell_type":"markdown","metadata":{"id":"1hmLgYGcbXTe"},"source":["# **Chapter 3: Data Loading and Preprocessing**\n","## **Overview**\n","\n","Loading and preprocessing data are critical steps in any machine learning workflow, especially in NLP. These steps ensure that the dataset is in the right format for the model to process effectively.\n","\n","**Loading the CoNLL2003 Dataset**\n","\n","The CoNLL2003 dataset is widely used for named entity recognition (NER), a common task in NLP where the goal is to identify and classify named entities in text into predefined categories such as the names of persons, organizations, locations, etc."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4157,"status":"ok","timestamp":1715499753056,"user":{"displayName":"Moiz Khan","userId":"06006918908994982853"},"user_tz":-300},"id":"eSW37HMMbL3j","outputId":"7c99ccb4-ee36-420f-ad9d-90e2fec65f2c"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]}],"source":["conll2003 = datasets.load_dataset(\"conll2003\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":33,"status":"ok","timestamp":1715499753056,"user":{"displayName":"Moiz Khan","userId":"06006918908994982853"},"user_tz":-300},"id":"ZmvZ-3lW64Rp","outputId":"fca3c3d5-64bd-443f-8700-a7638b92b856"},"outputs":[{"data":{"text/plain":["DatasetDict({\n","    train: Dataset({\n","        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n","        num_rows: 14041\n","    })\n","    validation: Dataset({\n","        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n","        num_rows: 3250\n","    })\n","    test: Dataset({\n","        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n","        num_rows: 3453\n","    })\n","})"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["conll2003"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":29,"status":"ok","timestamp":1715499753056,"user":{"displayName":"Moiz Khan","userId":"06006918908994982853"},"user_tz":-300},"id":"avns3yv47V8K","outputId":"2a71e94e-f845-42a8-b4b8-477c4c647695"},"outputs":[{"data":{"text/plain":["{'train': (14041, 5), 'validation': (3250, 5), 'test': (3453, 5)}"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["conll2003.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":27,"status":"ok","timestamp":1715499753056,"user":{"displayName":"Moiz Khan","userId":"06006918908994982853"},"user_tz":-300},"id":"G21zrHqK7WrT","outputId":"ff6e8a32-a4bf-4559-bea0-754e771f80ca"},"outputs":[{"data":{"text/plain":["{'id': '0',\n"," 'tokens': ['EU',\n","  'rejects',\n","  'German',\n","  'call',\n","  'to',\n","  'boycott',\n","  'British',\n","  'lamb',\n","  '.'],\n"," 'pos_tags': [22, 42, 16, 21, 35, 37, 16, 21, 7],\n"," 'chunk_tags': [11, 21, 11, 12, 21, 22, 11, 12, 0],\n"," 'ner_tags': [3, 0, 7, 0, 0, 0, 7, 0, 0]}"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["conll2003[\"train\"][0]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":27,"status":"ok","timestamp":1715499753057,"user":{"displayName":"Moiz Khan","userId":"06006918908994982853"},"user_tz":-300},"id":"Tf74JvNp7bEA","outputId":"b90ba97f-0d94-47af-c68e-0498848b25b5"},"outputs":[{"data":{"text/plain":["Sequence(feature=ClassLabel(names=['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC'], id=None), length=-1, id=None)"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["conll2003[\"train\"].features[\"ner_tags\"]"]},{"cell_type":"markdown","metadata":{"id":"r2aSYOOpgAds"},"source":["* O means the word doesnâ€™t correspond to any entity.\n","* B-PER/I-PER means the word corresponds to the beginning of/is inside a person entity.\n","* B-ORG/I-ORG means the word corresponds to the beginning of/is inside an organization entity.\n","* B-LOC/I-LOC means the word corresponds to the beginning of/is inside a location entity.\n","* B-MISC/I-MISC means the word corresponds to the beginning of/is inside a miscellaneous entity."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":157},"executionInfo":{"elapsed":25,"status":"ok","timestamp":1715499753057,"user":{"displayName":"Moiz Khan","userId":"06006918908994982853"},"user_tz":-300},"id":"u3LkjyaB7c9L","outputId":"16402482-a1a5-494b-a58f-e3ab52e0f87d"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'The shared task of CoNLL-2003 concerns language-independent named entity recognition. We will concentrate on\\nfour types of named entities: persons, locations, organizations and names of miscellaneous entities that do\\nnot belong to the previous three groups.\\n\\nThe CoNLL-2003 shared task data files contain four columns separated by a single space. Each word has been put on\\na separate line and there is an empty line after each sentence. The first item on each line is a word, the second\\na part-of-speech (POS) tag, the third a syntactic chunk tag and the fourth the named entity tag. The chunk tags\\nand the named entity tags have the format I-TYPE which means that the word is inside a phrase of type TYPE. Only\\nif two phrases of the same type immediately follow each other, the first word of the second phrase will have tag\\nB-TYPE to show that it starts a new phrase. A word with tag O is not part of a phrase. Note the dataset uses IOB2\\ntagging scheme, whereas the original dataset uses IOB1.\\n\\nFor more details see https://www.clips.uantwerpen.be/conll2003/ner/ and https://www.aclweb.org/anthology/W03-0419\\n'"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["conll2003['train'].description"]},{"cell_type":"markdown","metadata":{"id":"8TuvHTC8bvVW"},"source":["**Preprocessing for BERT**\n","\n","Preprocessing involves adapting the dataset to the format required by BERT for effective learning and prediction. This includes tokenization and aligning labels with BERT's token outputs.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"qQvT1c2DhI6r"},"source":["Tokenization in transformers like BERT involves splitting text into smaller pieces called tokens, which can be further divided into subwords. This can lead to a mismatch between the number of tokens and their corresponding labels. Here's how to align them effectively:\n","\n","1. **Special Tokens**: Tokens such as [CLS] and [SEP] are assigned a label of -100, indicating to PyTorch's CrossEntropyLoss function to exclude them from loss calculations. This ensures they do not affect the model training, as they donâ€™t represent actual words from the input.\n","\n","2. **Subword Tokens**: For words split into subwords, there are two labeling approaches:\n","   - **First Token Labeling**: Assign the label of the entire word to the first token and label the rest as -100. This focuses training on the first part of the word.\n","   - **Uniform Labeling**: Assign the same label to all subword tokens, treating each part equally in training.\n","\n","Using the label -100 helps to focus learning on meaningful tokens and prevents non-representative tokens from influencing the modelâ€™s performance."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":24,"status":"ok","timestamp":1715499753057,"user":{"displayName":"Moiz Khan","userId":"06006918908994982853"},"user_tz":-300},"id":"Quls3AG-7gr5","outputId":"6c9cf4bc-7d44-42cd-9aaa-4bf855839634"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n","  warnings.warn(\n"]}],"source":["tokenizer = BertTokenizerFast.from_pretrained(\"nlpaueb/legal-bert-base-uncased\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":22,"status":"ok","timestamp":1715499753057,"user":{"displayName":"Moiz Khan","userId":"06006918908994982853"},"user_tz":-300},"id":"pnR8Jz_07jac","outputId":"8bb48c14-6aad-49a9-9a18-828a41857a59"},"outputs":[{"name":"stdout","output_type":"stream","text":["[None, 0, 1, 2, 3, 4, 5, 6, 6, 7, 8, None]\n"]},{"data":{"text/plain":["{'input_ids': [101, 501, 5714, 1600, 1842, 211, 21215, 3585, 178, 7846, 117, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["example_text = conll2003['train'][0]\n","\n","tokenized_input = tokenizer(example_text[\"tokens\"], is_split_into_words=True)\n","\n","tokens = tokenizer.convert_ids_to_tokens(tokenized_input[\"input_ids\"])\n","\n","word_ids = tokenized_input.word_ids()\n","\n","print(word_ids)\n","\n","\n","''' As we can see, it returns a list with the same number of elements as our processed input ids, mapping special tokens to None and all other tokens to their respective word. This way, we can align the labels with the processed input ids. '''\n","\n","tokenized_input"]},{"cell_type":"markdown","metadata":{"id":"KUCW_KRmcO9b"},"source":["The **word_ids** function maps tokens to their original words (necessary because BERT tokenizer can split words into subwords). This mapping is used to ensure that labels correspond correctly to their respective tokens, an essential step for training the model on tasks like NER."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":20,"status":"ok","timestamp":1715499753057,"user":{"displayName":"Moiz Khan","userId":"06006918908994982853"},"user_tz":-300},"id":"excsGzM97mQO","outputId":"afcdecaf-bd69-43ca-9af2-630a23f7387e"},"outputs":[{"data":{"text/plain":["(9, 12)"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["len(example_text['ner_tags']), len(tokenized_input[\"input_ids\"])\n","# (9, 11)"]},{"cell_type":"markdown","metadata":{"id":"kCbIFF5vhr2H"},"source":["The below function prepares text for training by aligning labels with tokens in two key ways:\n","\n","Ignoring Unnecessary Tokens: It sets the label -100 for special tokens like [CLS] and [SEP] and for any additional subword parts after the first one. This tells the training process to ignore these tokens because they don't correspond to real data or they're less relevant.\n","\n","Aligning Labels: It ensures each token that should be considered during training has the correct label from the original data. If a word is split into multiple tokens, depending on the chosen strategy, either only the first token or all tokens are assigned the original wordâ€™s label."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rMa8LKF07o5J"},"outputs":[],"source":["def tokenize_and_align_labels(examples, label_all_tokens=True):\n","    \"\"\"\n","    Function to tokenize and align labels with respect to the tokens. This function is specifically designed for\n","    Named Entity Recognition (NER) tasks where alignment of the labels is necessary after tokenization.\n","\n","    Parameters:\n","    examples (dict): A dictionary containing the tokens and the corresponding NER tags.\n","                     - \"tokens\": list of words in a sentence.\n","                     - \"ner_tags\": list of corresponding entity tags for each word.\n","\n","    label_all_tokens (bool): A flag to indicate whether all tokens should have labels.\n","                             If False, only the first token of a word will have a label,\n","                             the other tokens (subwords) corresponding to the same word will be assigned -100.\n","\n","    Returns:\n","    tokenized_inputs (dict): A dictionary containing the tokenized inputs and the corresponding labels aligned with the tokens.\n","    \"\"\"\n","    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True)\n","    labels = []\n","    for i, label in enumerate(examples[\"ner_tags\"]):\n","        word_ids = tokenized_inputs.word_ids(batch_index=i)\n","        # word_ids() => Return a list mapping the tokens\n","        # to their actual word in the initial sentence.\n","        # It Returns a list indicating the word corresponding to each token.\n","        previous_word_idx = None\n","        label_ids = []\n","        # Special tokens like `<s>` and `<\\s>` are originally mapped to None\n","        # We need to set the label to -100 so they are automatically ignored in the loss function.\n","        for word_idx in word_ids:\n","            if word_idx is None:\n","                # set â€“100 as the label for these special tokens\n","                label_ids.append(-100)\n","            # For the other tokens in a word, we set the label to either the current label or -100, depending on\n","            # the label_all_tokens flag.\n","            elif word_idx != previous_word_idx:\n","                # if current word_idx is != prev then its the most regular case\n","                # and add the corresponding token\n","                label_ids.append(label[word_idx])\n","            else:\n","                # to take care of sub-words which have the same word_idx\n","                # set -100 as well for them, but only if label_all_tokens == False\n","                label_ids.append(label[word_idx] if label_all_tokens else -100)\n","                # mask the subword representations after the first subword\n","\n","            previous_word_idx = word_idx\n","        labels.append(label_ids)\n","    tokenized_inputs[\"labels\"] = labels\n","    return tokenized_inputs"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":19,"status":"ok","timestamp":1715499753058,"user":{"displayName":"Moiz Khan","userId":"06006918908994982853"},"user_tz":-300},"id":"RdLYDYw87tVu","outputId":"7c76ad1f-7a21-4604-f13c-6d95b7d1c0b7"},"outputs":[{"name":"stdout","output_type":"stream","text":["{'input_ids': [[101, 1221, 110, 163, 900, 211, 207, 274, 403, 110, 163, 2824, 195, 526, 532, 188, 3298, 13898, 235, 4149, 786, 222, 15034, 2765, 305, 2778, 4899, 3681, 238, 779, 231, 268, 6852, 890, 598, 207, 2137, 2580, 246, 969, 577, 117, 102]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], 'labels': [[-100, 5, 0, 0, 0, 0, 0, 3, 4, 0, 0, 0, 0, 0, 1, 1, 1, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 5, 0, 0, 0, 0, 0, 0, 0, 0, -100]]}\n"]}],"source":["q = tokenize_and_align_labels(conll2003['train'][4:5])\n","print(q)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":17,"status":"ok","timestamp":1715499753058,"user":{"displayName":"Moiz Khan","userId":"06006918908994982853"},"user_tz":-300},"id":"FkehN0kn7zvU","outputId":"4f7f4ccf-c0a0-4e5c-fd51-56de2511a7ec"},"outputs":[{"name":"stdout","output_type":"stream","text":["[CLS]___________________________________ -100\n","germany_________________________________ 5\n","'_______________________________________ 0\n","s_______________________________________ 0\n","representative__________________________ 0\n","to______________________________________ 0\n","the_____________________________________ 0\n","european________________________________ 3\n","union___________________________________ 4\n","'_______________________________________ 0\n","s_______________________________________ 0\n","veterinar_______________________________ 0\n","##y_____________________________________ 0\n","committee_______________________________ 0\n","we______________________________________ 1\n","##r_____________________________________ 1\n","##ner___________________________________ 1\n","zw______________________________________ 2\n","##ing___________________________________ 2\n","##mann__________________________________ 2\n","said____________________________________ 0\n","on______________________________________ 0\n","wednesday_______________________________ 0\n","consumers_______________________________ 0\n","should__________________________________ 0\n","buy_____________________________________ 0\n","sheep___________________________________ 0\n","##meat__________________________________ 0\n","from____________________________________ 0\n","countries_______________________________ 0\n","other___________________________________ 0\n","than____________________________________ 0\n","brita___________________________________ 5\n","##in____________________________________ 5\n","until___________________________________ 0\n","the_____________________________________ 0\n","scientific______________________________ 0\n","advice__________________________________ 0\n","was_____________________________________ 0\n","clear___________________________________ 0\n","##er____________________________________ 0\n","._______________________________________ 0\n","[SEP]___________________________________ -100\n"]}],"source":["for token, label in zip(tokenizer.convert_ids_to_tokens(q[\"input_ids\"][0]),q[\"labels\"][0]):\n","    print(f\"{token:_<40} {label}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gJtTE6Qr71ei"},"outputs":[],"source":["tokenized_datasets = conll2003.map(tokenize_and_align_labels, batched=True)"]},{"cell_type":"markdown","metadata":{"id":"-smo3CWXdCaQ"},"source":["# **Chapter 4: Model Configuration and Fine-tuning**\n","## **Overview**\n","\n","Model configuration involves setting up the BERT model for the specific task of token classification. Fine-tuning is the process of adapting a pre-trained model to a specific dataset or task by continuing the training process with task-specific data.\n","\n","\n","**Setting up the Model**\n","\n","The model used is a variant of BERT, specifically adapted for legal texts (nlpaueb/legal-bert-base-uncased), making it highly relevant for datasets involving legal or formal language."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":5921,"status":"ok","timestamp":1715499759525,"user":{"displayName":"Moiz Khan","userId":"06006918908994982853"},"user_tz":-300},"id":"dciPOpFD73P5","outputId":"7ec3bf64-0a94-414d-a6e2-26af177380a5"},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of BertForTokenClassification were not initialized from the model checkpoint at nlpaueb/legal-bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}],"source":["model = AutoModelForTokenClassification.from_pretrained(\"nlpaueb/legal-bert-base-uncased\", num_labels=9)"]},{"cell_type":"markdown","metadata":{"id":"fcyIUNWQdZkN"},"source":["This code initializes a BERT model for token classification with the number of labels equal to the number of named entity types in the CoNLL2003 dataset. The model is loaded with weights pre-trained on legal text data, offering a robust starting point for further training."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":11260,"status":"ok","timestamp":1715499770764,"user":{"displayName":"Moiz Khan","userId":"06006918908994982853"},"user_tz":-300},"id":"OWCkVF7OX1RH","outputId":"4f588759-3c97-4849-d8d2-c2d2b21584c0"},"outputs":[],"source":["pip install accelerate -U"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MZODsiJO75wJ"},"outputs":[],"source":["from transformers import TrainingArguments, Trainer\n","args = TrainingArguments(\n","\"test-ner\",\n","evaluation_strategy = \"epoch\",\n","learning_rate=2e-5,\n","per_device_train_batch_size=16,\n","per_device_eval_batch_size=16,\n","num_train_epochs=3,\n","weight_decay=0.01,\n",")"]},{"cell_type":"markdown","metadata":{"id":"Ij0AsbrReAk7"},"source":["This block sets various training parameters like batch size, learning rate, and the number of epochs. These parameters are crucial for controlling the training process and ensuring the model learns effectively without overfitting."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HEVbh4jR8MWF"},"outputs":[],"source":["data_collator = DataCollatorForTokenClassification(tokenizer)"]},{"cell_type":"markdown","metadata":{"id":"Rcocexsddg1Y"},"source":["A data collator prepares batches of data, handling token padding and creating attention masks necessary for training the model efficiently."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":173,"referenced_widgets":["32bbbee58a9749a5b019b8fdcab6914f","f61c2f1e88ef4cdf8bcf4b539fc0b821","cb94b13dadec453fb9ba6c675ba9bae0","800102930179483ba562f23057543eef","8329f16c3af648e8bf744ef81adaf0b0","e0ad809cb1ff42b1bbef5ee3e7cf456a","9bf6a2d32f6546a9a120615d70dec61a","2fee6d86025344ae9926f2dc41b7ea72","2ab3b64a280c48f393ab020f5edf1022","ba2b8f271a7948dabae928c2a453c8c3","03cac395d3494c9f90df8fd9d3a07fec"]},"collapsed":true,"executionInfo":{"elapsed":1170,"status":"ok","timestamp":1715499771916,"user":{"displayName":"Moiz Khan","userId":"06006918908994982853"},"user_tz":-300},"id":"rzSYQte-8PZo","outputId":"7bf2624f-336c-479f-95f4-12e5976cc980"},"outputs":[{"name":"stderr","output_type":"stream","text":["<ipython-input-20-b144b02b338d>:1: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n","  metric = datasets.load_metric(\"seqeval\")\n","/usr/local/lib/python3.10/dist-packages/datasets/load.py:759: FutureWarning: The repository for seqeval contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.1/metrics/seqeval/seqeval.py\n","You can avoid this message in future by passing the argument `trust_remote_code=True`.\n","Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n","  warnings.warn(\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"32bbbee58a9749a5b019b8fdcab6914f","version_major":2,"version_minor":0},"text/plain":["Downloading builder script:   0%|          | 0.00/2.47k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"}],"source":["metric = datasets.load_metric(\"seqeval\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Q52Juqlo8QvA"},"outputs":[],"source":["example = conll2003['train'][0]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":27,"status":"ok","timestamp":1715499771917,"user":{"displayName":"Moiz Khan","userId":"06006918908994982853"},"user_tz":-300},"id":"W-x4ik5F8S3U","outputId":"a74a6893-9a2a-494c-a02b-20118be09b2a"},"outputs":[{"data":{"text/plain":["['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC']"]},"execution_count":22,"metadata":{},"output_type":"execute_result"}],"source":["label_list = conll2003[\"train\"].features[\"ner_tags\"].feature.names\n","\n","label_list"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":22,"status":"ok","timestamp":1715499771917,"user":{"displayName":"Moiz Khan","userId":"06006918908994982853"},"user_tz":-300},"id":"LxDrhfcD8UjT","outputId":"0a168f50-e6ca-419a-a81e-3c31ff45854b"},"outputs":[{"data":{"text/plain":["{'MISC': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'number': 2},\n"," 'ORG': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'number': 1},\n"," 'overall_precision': 1.0,\n"," 'overall_recall': 1.0,\n"," 'overall_f1': 1.0,\n"," 'overall_accuracy': 1.0}"]},"execution_count":23,"metadata":{},"output_type":"execute_result"}],"source":["labels = [label_list[i] for i in example[\"ner_tags\"]]\n","\n","metric.compute(predictions=[labels], references=[labels])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"waV4gl-C8V7U"},"outputs":[],"source":["def compute_metrics(eval_preds):\n","    \"\"\"\n","    Function to compute the evaluation metrics for Named Entity Recognition (NER) tasks.\n","    The function computes precision, recall, F1 score and accuracy.\n","\n","    Parameters:\n","    eval_preds (tuple): A tuple containing the predicted logits and the true labels.\n","\n","    Returns:\n","    A dictionary containing the precision, recall, F1 score and accuracy.\n","    \"\"\"\n","    pred_logits, labels = eval_preds\n","\n","    pred_logits = np.argmax(pred_logits, axis=2)\n","    # the logits and the probabilities are in the same order,\n","    # so we donâ€™t need to apply the softmax\n","\n","    # We remove all the values where the label is -100\n","    predictions = [\n","        [label_list[eval_preds] for (eval_preds, l) in zip(prediction, label) if l != -100]\n","        for prediction, label in zip(pred_logits, labels)\n","    ]\n","\n","    true_labels = [\n","      [label_list[l] for (eval_preds, l) in zip(prediction, label) if l != -100]\n","       for prediction, label in zip(pred_logits, labels)\n","   ]\n","    results = metric.compute(predictions=predictions, references=true_labels)\n","    return {\n","   \"precision\": results[\"overall_precision\"],\n","   \"recall\": results[\"overall_recall\"],\n","   \"f1\": results[\"overall_f1\"],\n","  \"accuracy\": results[\"overall_accuracy\"],\n","  }"]},{"cell_type":"markdown","metadata":{"id":"own3ofeBiT4l"},"source":["This compute_metrics() function first takes the argmax of the logits to convert them to predictions (as usual, the logits and the probabilities are in the same order, so we donâ€™t need to apply the softmax). Then we have to convert both labels and predictions from integers to strings. We remove all the values where the label is -100, then pass the results to the metric.compute() method"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"317OScUg8dHi"},"outputs":[],"source":["trainer = Trainer(\n","    model,\n","    args,\n","   train_dataset=tokenized_datasets[\"train\"],\n","   eval_dataset=tokenized_datasets[\"validation\"],\n","   data_collator=data_collator,\n","   tokenizer=tokenizer,\n","   compute_metrics=compute_metrics\n",")"]},{"cell_type":"markdown","metadata":{"id":"e34f-MyqeI02"},"source":["The **Trainer** from Hugging Faceâ€™s Transformers library handles the training process. It leverages the training arguments, model, datasets, and tokenizer to manage the training loop, including backpropagation and evaluation."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":204},"executionInfo":{"elapsed":548484,"status":"ok","timestamp":1715500320945,"user":{"displayName":"Moiz Khan","userId":"06006918908994982853"},"user_tz":-300},"id":"7Nh-k9jh8f7q","outputId":"ac736715-3a11-4381-fef0-8ee01be96948"},"outputs":[{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='2634' max='2634' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [2634/2634 09:06, Epoch 3/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>F1</th>\n","      <th>Accuracy</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.314500</td>\n","      <td>0.105518</td>\n","      <td>0.871666</td>\n","      <td>0.884512</td>\n","      <td>0.878042</td>\n","      <td>0.968774</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.087200</td>\n","      <td>0.089533</td>\n","      <td>0.905817</td>\n","      <td>0.902123</td>\n","      <td>0.903966</td>\n","      <td>0.975013</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.053800</td>\n","      <td>0.088669</td>\n","      <td>0.906511</td>\n","      <td>0.914913</td>\n","      <td>0.910693</td>\n","      <td>0.976190</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["TrainOutput(global_step=2634, training_loss=0.12611545372298927, metrics={'train_runtime': 548.0854, 'train_samples_per_second': 76.855, 'train_steps_per_second': 4.806, 'total_flos': 1101309468061158.0, 'train_loss': 0.12611545372298927, 'epoch': 3.0})"]},"execution_count":26,"metadata":{},"output_type":"execute_result"}],"source":["trainer.train()"]},{"cell_type":"markdown","metadata":{"id":"oGMyV0-ZfPxd"},"source":["Model was evaluated  using metrics that are relevant to the NER task, such as precision, recall, and the F1 score, which provide a balanced measure of the model's accuracy and its ability to handle all classes in the dataset."]},{"cell_type":"markdown","metadata":{"id":"CxyXzpXXj7Zn"},"source":["\n","\n","# **Chapter 6: Conclusion and Further Steps**\n","\n","## **Overview**\n","\n","This final chapter summarizes the project, reflects on the lessons learned, and outlines potential areas for future research or improvement based on the results obtained from fine-tuning the BERT model for named entity recognition (NER).\n","\n","**Project Summary**\n","\n","Throughout this project, we've taken significant steps in advancing NLP tasks using a fine-tuned BERT model. Starting with an introduction to the BERT framework, we prepared the environment and datasets for training, set up and fine-tuned the model, and evaluated its performance rigorously through multiple metrics. The project demonstrated not only the power of transformer models in handling complex NLP tasks but also the importance of meticulous data preprocessing and model tuning.\n","\n","**Achievements**\n","\n","- **Model Performance**: The model achieved impressive precision, recall, F1 score, and accuracy, showing strong predictive capabilities on the CoNLL2003 dataset.\n","- **Understanding of BERT**: Gained a deep understanding of how BERT processes language data and the implications of its bidirectional nature and subword tokenization strategy.\n","- **Technical Skills**: Enhanced skills in Python, PyTorch, and the Hugging Face Transformers library, which are crucial for NLP tasks.\n","\n","**Challenges**\n","\n","- **Data Alignment**: Addressing issues related to tokenization and alignment of labels with BERTâ€™s subword tokens was challenging and required careful handling to ensure data integrity.\n","- **Model Tuning**: Deciding on the best model configuration and stopping criteria to avoid overfitting while maintaining high performance on unseen data.\n","\n","\n","---\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DQadtD0l8hKt"},"outputs":[],"source":["model.save_pretrained(\"ner_model\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11,"status":"ok","timestamp":1715500323416,"user":{"displayName":"Moiz Khan","userId":"06006918908994982853"},"user_tz":-300},"id":"rxi6qHOL8icT","outputId":"4a889a54-8b0d-47b1-9f9b-de2b7b5cf6e1"},"outputs":[{"data":{"text/plain":["('tokenizer/tokenizer_config.json',\n"," 'tokenizer/special_tokens_map.json',\n"," 'tokenizer/vocab.txt',\n"," 'tokenizer/added_tokens.json',\n"," 'tokenizer/tokenizer.json')"]},"execution_count":28,"metadata":{},"output_type":"execute_result"}],"source":["tokenizer.save_pretrained(\"tokenizer\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JbINWBpq8jyN"},"outputs":[],"source":["id2label = {\n","    str(i): label for i,label in enumerate(label_list)\n","\n","}\n","label2id = {\n","    label: str(i) for i,label in enumerate(label_list)\n","}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Y9l6WN038lNz"},"outputs":[],"source":["import json"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"M7fBnBuX8n_U"},"outputs":[],"source":["config = json.load(open(\"ner_model/config.json\"))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dRTmXOfn8pTk"},"outputs":[],"source":["config[\"id2label\"] = id2label\n","config[\"label2id\"] = label2id"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"33uenWui8qiL"},"outputs":[],"source":["json.dump(config, open(\"ner_model/config.json\",\"w\"))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iIe_b7s38roP"},"outputs":[],"source":["model_fine_tuned = AutoModelForTokenClassification.from_pretrained(\"ner_model\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":701,"status":"ok","timestamp":1715500324682,"user":{"displayName":"Moiz Khan","userId":"06006918908994982853"},"user_tz":-300},"id":"AimXI2vk8vG5","outputId":"7efd7d71-8ee4-4887-ee65-4c9d0c136d76"},"outputs":[{"name":"stdout","output_type":"stream","text":["  Index  Word    Entity       Score\n","-------  ------  --------  --------\n","      1  bill    B-PER     0.997214\n","      2  gate    I-PER     0.997775\n","      3  ##s     I-PER     0.997131\n","      8  micro   B-ORG     0.988478\n","      9  ##soft  B-ORG     0.978292\n"]}],"source":["from transformers import pipeline\n","from tabulate import tabulate\n","\n","nlp = pipeline(\"ner\", model=model_fine_tuned, tokenizer=tokenizer)\n","\n","example = \"Bill Gates is the Founder of Microsoft\"\n","\n","ner_results = nlp(example)\n","\n","table = [[\"Index\", \"Word\", \"Entity\", \"Score\"]]\n","for item in ner_results:\n","    table.append([item[\"index\"], item[\"word\"], item[\"entity\"], item[\"score\"]])\n","\n","print(tabulate(table, headers=\"firstrow\"))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":384},"executionInfo":{"elapsed":24,"status":"error","timestamp":1715500324683,"user":{"displayName":"Moiz Khan","userId":"06006918908994982853"},"user_tz":-300},"id":"Hq9fFRoubzka","outputId":"115e762b-098a-49cc-a086-c374bc402c3c"},"outputs":[],"source":["import fitz  # Library which helps in working with PDF files\n","\n","def convert_pdf_to_text(pdf_path):\n","    \"\"\"Converts a PDF file to plain text.\n","\n","    Args:\n","        pdf_path (str): The file path of the PDF document.\n","\n","    Returns:\n","        str: The extracted text from the PDF.\n","    \"\"\"\n","    doc = fitz.open(pdf_path)  # Open the PDF file\n","    text = ''  # Initialize an empty string to store the extracted text\n","    for page in doc:  # Loop through each page in the PDF\n","        text += page.get_text()  # Extract text from the current page and add it to the text variable\n","    return text  # Return the collected text\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"upmj3EjIdY0i"},"outputs":[],"source":["convert_pdf_to_text(\"/content/crl.a._3_q_2021.pdf\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"a8I0lSJ9dmXe"},"outputs":[],"source":["ner_results = nlp(convert_pdf_to_text(\"/content/crl.a._3_q_2021.pdf\"))\n","\n","table = [[\"Index\", \"Word\", \"Entity\", \"Score\"]]\n","for item in ner_results:\n","    table.append([item[\"index\"], item[\"word\"], item[\"entity\"], item[\"score\"]])\n","\n","print(tabulate(table, headers=\"firstrow\"))\n"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[{"file_id":"1msng4es7FztKJj0e_aiqhNHdMk8RYL1c","timestamp":1715502288683}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.4"},"widgets":{"application/vnd.jupyter.widget-state+json":{"03cac395d3494c9f90df8fd9d3a07fec":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2ab3b64a280c48f393ab020f5edf1022":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"2fee6d86025344ae9926f2dc41b7ea72":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"32bbbee58a9749a5b019b8fdcab6914f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_f61c2f1e88ef4cdf8bcf4b539fc0b821","IPY_MODEL_cb94b13dadec453fb9ba6c675ba9bae0","IPY_MODEL_800102930179483ba562f23057543eef"],"layout":"IPY_MODEL_8329f16c3af648e8bf744ef81adaf0b0"}},"800102930179483ba562f23057543eef":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ba2b8f271a7948dabae928c2a453c8c3","placeholder":"â€‹","style":"IPY_MODEL_03cac395d3494c9f90df8fd9d3a07fec","value":"â€‡6.33k/?â€‡[00:00&lt;00:00,â€‡92.4kB/s]"}},"8329f16c3af648e8bf744ef81adaf0b0":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9bf6a2d32f6546a9a120615d70dec61a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ba2b8f271a7948dabae928c2a453c8c3":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cb94b13dadec453fb9ba6c675ba9bae0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_2fee6d86025344ae9926f2dc41b7ea72","max":2471,"min":0,"orientation":"horizontal","style":"IPY_MODEL_2ab3b64a280c48f393ab020f5edf1022","value":2471}},"e0ad809cb1ff42b1bbef5ee3e7cf456a":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f61c2f1e88ef4cdf8bcf4b539fc0b821":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e0ad809cb1ff42b1bbef5ee3e7cf456a","placeholder":"â€‹","style":"IPY_MODEL_9bf6a2d32f6546a9a120615d70dec61a","value":"Downloadingâ€‡builderâ€‡script:â€‡"}}}}},"nbformat":4,"nbformat_minor":0}
